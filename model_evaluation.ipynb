{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7bb90b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Package cuDNN not found in current path.\n",
      "│ - Run `import Pkg; Pkg.add(\"cuDNN\")` to install the cuDNN package, then restart julia.\n",
      "│ - If cuDNN is not installed, some Flux functionalities will not be available when running on the GPU.\n",
      "└ @ FluxCUDAExt C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\ext\\FluxCUDAExt\\FluxCUDAExt.jl:10\n"
     ]
    }
   ],
   "source": [
    "using Flux, ParameterSchedulers, Optimisers, Statistics, CUDA \n",
    "using Flux: Conv, BatchNorm, MaxPool, flatten, Dense, Dropout, relu, softmax\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON, JLD2, LinearAlgebra\n",
    "using ImageCore, Images\n",
    "using MLDatasets: convert2image, CIFAR10\n",
    "using Plots              \n",
    "using StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c33258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"true\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eafb28f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297bac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training on CPU\n",
      "└ @ Main c:\\Users\\oliwi\\Desktop\\SGH\\masgisterka\\II_sem\\Deep Learning\\projekt_zaliczeniowy\\DeepLearningProjektZaliczeniowy\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:6\n"
     ]
    }
   ],
   "source": [
    "if use_cuda && CUDA.functional()\n",
    "    device = gpu\n",
    "    @info \"Training on GPU\"\n",
    "else\n",
    "    device = cpu\n",
    "    @info \"Training on CPU\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84bd061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_minibatch (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, 32, 32, 3, length(idxs))\n",
    "    for (i, idx) in enumerate(idxs)\n",
    "        X_batch[:,:,:,i] = X[:,:,:,idx]\n",
    "    end\n",
    "    Y_batch = Flux.onehotbatch(Y[idxs], 0:9)\n",
    "    return X_batch, Y_batch\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc2f00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.61960787 0.59607846 … 0.23921569 0.21176471; 0.62352943 0.5921569 … 0.19215687 0.21960784; … ; 0.49411765 0.49019608 … 0.11372549 0.13333334; 0.45490196 0.46666667 … 0.078431375 0.08235294;;; 0.4392157 0.4392157 … 0.45490196 0.41960785; 0.43529412 0.43137255 … 0.4 0.4117647; … ; 0.35686275 0.35686275 … 0.32156864 0.32941177; 0.33333334 0.34509805 … 0.2509804 0.2627451;;; 0.19215687 0.2 … 0.65882355 0.627451; 0.18431373 0.15686275 … 0.5803922 0.58431375; … ; 0.14117648 0.1254902 … 0.49411765 0.5058824; 0.12941177 0.13333334 … 0.41960785 0.43137255;;;; 0.92156863 0.93333334 … 0.32156864 0.33333334; 0.90588236 0.92156863 … 0.18039216 0.24313726; … ; 0.9137255 0.9254902 … 0.7254902 0.7058824; 0.9098039 0.92156863 … 0.73333335 0.7294118;;; 0.92156863 0.93333334 … 0.3764706 0.39607844; 0.90588236 0.92156863 … 0.22352941 0.29411766; … ; 0.9137255 0.9254902 … 0.78431374 0.7647059; 0.9098039 0.92156863 … 0.7921569 0.78431374;;; 0.92156863 0.93333334 … 0.32156864 0.3254902; 0.90588236 0.92156863 … 0.14117648 0.1882353; … ; 0.9137255 0.9254902 … 0.76862746 0.7490196; 0.9098039 0.92156863 … 0.78431374 0.78039217;;;; 0.61960787 0.6666667 … 0.09019608 0.10980392; 0.61960787 0.6745098 … 0.105882354 0.11764706; … ; 0.92941177 0.9647059 … 0.015686275 0.015686275; 0.93333334 0.9647059 … 0.019607844 0.02745098;;; 0.74509805 0.78431374 … 0.13333334 0.16078432; 0.73333335 0.78039217 … 0.14901961 0.16862746; … ; 0.9372549 0.9647059 … 0.023529412 0.019607844; 0.94509804 0.96862745 … 0.02745098 0.03137255;;; 0.87058824 0.8980392 … 0.15294118 0.18431373; 0.85490197 0.8862745 … 0.16862746 0.19607843; … ; 0.9529412 0.98039216 … 0.011764706 0.011764706; 0.9647059 0.9843137 … 0.011764706 0.02745098;;;; … ;;;; 0.078431375 0.08235294 … 0.12941177 0.12156863; 0.07450981 0.078431375 … 0.13333334 0.1254902; … ; 0.047058824 0.039215688 … 0.105882354 0.101960786; 0.050980393 0.047058824 … 0.09803922 0.09803922;;; 0.05882353 0.0627451 … 0.09803922 0.09019608; 0.05490196 0.0627451 … 0.101960786 0.09411765; … ; 0.043137256 0.03529412 … 0.09411765 0.09019608; 0.047058824 0.043137256 … 0.08627451 0.078431375;;; 0.047058824 0.050980393 … 0.05490196 0.047058824; 0.043137256 0.050980393 … 0.05882353 0.050980393; … ; 0.03529412 0.02745098 … 0.21960784 0.20784314; 0.039215688 0.03529412 … 0.18431373 0.18431373;;;; 0.09803922 0.047058824 … 0.40392157 0.37254903; 0.05882353 0.078431375 … 0.40784314 0.37254903; … ; 0.36078432 0.58431375 … 0.3882353 0.37254903; 0.29411766 0.40784314 … 0.36078432 0.36078432;;; 0.15686275 0.09803922 … 0.5176471 0.49411765; 0.14117648 0.14509805 … 0.5137255 0.48235294; … ; 0.44313726 0.65882355 … 0.49803922 0.48235294; 0.34901962 0.45882353 … 0.4745098 0.47058824;;; 0.047058824 0.023529412 … 0.3254902 0.30588236; 0.011764706 0.02745098 … 0.3254902 0.29803923; … ; 0.4392157 0.69411767 … 0.32941177 0.31764707; 0.36078432 0.5137255 … 0.30980393 0.3137255;;;; 0.28627452 0.27058825 … 0.4509804 0.45490196; 0.38431373 0.32941177 … 0.48235294 0.4745098; … ; 0.5294118 0.2784314 … 0.25882354 0.26666668; 0.79607844 0.47058824 … 0.105882354 0.105882354;;; 0.30588236 0.28627452 … 0.4745098 0.47058824; 0.40392157 0.34901962 … 0.4862745 0.47843137; … ; 0.58431375 0.32156864 … 0.25490198 0.25490198; 0.84313726 0.52156866 … 0.105882354 0.101960786;;; 0.29411766 0.27450982 … 0.35686275 0.3529412; 0.44313726 0.38039216 … 0.37254903 0.36862746; … ; 0.6039216 0.3137255 … 0.23137255 0.22745098; 0.8745098 0.5294118 … 0.105882354 0.101960786], Bool[0 0 … 0 0; 0 0 … 1 0; … ; 0 1 … 0 0; 0 0 … 0 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_set = CIFAR10(:test)\n",
    "test_set = make_minibatch(test_set.features, test_set.targets, 1:size(test_set.features)[4]) |> device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44d0c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Conv((3, 3), 3 => 16, pad=1),         \u001b[90m# 448 parameters\u001b[39m\n",
       "  BatchNorm(16),                        \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 32\u001b[39m\n",
       "  NNlib.relu,\n",
       "  MaxPool((2, 2)),\n",
       "  Conv((3, 3), 16 => 32, pad=1),        \u001b[90m# 4_640 parameters\u001b[39m\n",
       "  BatchNorm(32),                        \u001b[90m# 64 parameters\u001b[39m\u001b[90m, plus 64\u001b[39m\n",
       "  NNlib.relu,\n",
       "  MaxPool((2, 2)),\n",
       "  Conv((3, 3), 32 => 64, pad=1),        \u001b[90m# 18_496 parameters\u001b[39m\n",
       "  BatchNorm(64),                        \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\n",
       "  NNlib.relu,\n",
       "  MaxPool((2, 2)),\n",
       "  Flux.flatten,\n",
       "  Dense(1024 => 128),                   \u001b[90m# 131_200 parameters\u001b[39m\n",
       "  NNlib.relu,\n",
       "  Dropout(0.5),\n",
       "  Dense(128 => 10),                     \u001b[90m# 1_290 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m        # Total: 16 trainable arrays, \u001b[39m156_298 parameters,\n",
       "\u001b[90m          # plus 6 non-trainable, 224 parameters, summarysize \u001b[39m613.047 KiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_small_convnet = Chain(\n",
    "  # — blok 1: 32×32 → 16×16, kanały 3 → 16 —\n",
    "  Conv((3,3), 3=>16, pad=1), BatchNorm(16), relu,\n",
    "  MaxPool((2,2)),                           # 16×16\n",
    "\n",
    "  # — blok 2: 16×16 → 8×8, kanały 16 → 32 —\n",
    "  Conv((3,3), 16=>32, pad=1), BatchNorm(32), relu,\n",
    "  MaxPool((2,2)),                           # 8×8\n",
    "\n",
    "  # — blok 3: 8×8 → 4×4, kanały 32 → 64 —\n",
    "  Conv((3,3), 32=>64, pad=1), BatchNorm(64), relu,\n",
    "  MaxPool((2,2)),                           # 4×4\n",
    "\n",
    "  # — flatten + head FC —\n",
    "  flatten,                                  # 64*4*4 = 1024\n",
    "  Dense(1024, 128), relu, Dropout(0.5),\n",
    "  Dense(128,   10),\n",
    "  softmax\n",
    ") |> device\n",
    "\n",
    "BSON.@load \"model_small_convnet.bson\" ps\n",
    "\n",
    "Flux.loadmodel!(model_small_convnet, device(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950d7adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.61960787 0.59607846 … 0.23921569 0.21176471; 0.62352943 0.5921569 … 0.19215687 0.21960784; … ; 0.49411765 0.49019608 … 0.11372549 0.13333334; 0.45490196 0.46666667 … 0.078431375 0.08235294;;; 0.4392157 0.4392157 … 0.45490196 0.41960785; 0.43529412 0.43137255 … 0.4 0.4117647; … ; 0.35686275 0.35686275 … 0.32156864 0.32941177; 0.33333334 0.34509805 … 0.2509804 0.2627451;;; 0.19215687 0.2 … 0.65882355 0.627451; 0.18431373 0.15686275 … 0.5803922 0.58431375; … ; 0.14117648 0.1254902 … 0.49411765 0.5058824; 0.12941177 0.13333334 … 0.41960785 0.43137255;;;; 0.92156863 0.93333334 … 0.32156864 0.33333334; 0.90588236 0.92156863 … 0.18039216 0.24313726; … ; 0.9137255 0.9254902 … 0.7254902 0.7058824; 0.9098039 0.92156863 … 0.73333335 0.7294118;;; 0.92156863 0.93333334 … 0.3764706 0.39607844; 0.90588236 0.92156863 … 0.22352941 0.29411766; … ; 0.9137255 0.9254902 … 0.78431374 0.7647059; 0.9098039 0.92156863 … 0.7921569 0.78431374;;; 0.92156863 0.93333334 … 0.32156864 0.3254902; 0.90588236 0.92156863 … 0.14117648 0.1882353; … ; 0.9137255 0.9254902 … 0.76862746 0.7490196; 0.9098039 0.92156863 … 0.78431374 0.78039217;;;; 0.61960787 0.6666667 … 0.09019608 0.10980392; 0.61960787 0.6745098 … 0.105882354 0.11764706; … ; 0.92941177 0.9647059 … 0.015686275 0.015686275; 0.93333334 0.9647059 … 0.019607844 0.02745098;;; 0.74509805 0.78431374 … 0.13333334 0.16078432; 0.73333335 0.78039217 … 0.14901961 0.16862746; … ; 0.9372549 0.9647059 … 0.023529412 0.019607844; 0.94509804 0.96862745 … 0.02745098 0.03137255;;; 0.87058824 0.8980392 … 0.15294118 0.18431373; 0.85490197 0.8862745 … 0.16862746 0.19607843; … ; 0.9529412 0.98039216 … 0.011764706 0.011764706; 0.9647059 0.9843137 … 0.011764706 0.02745098;;;; … ;;;; 0.078431375 0.08235294 … 0.12941177 0.12156863; 0.07450981 0.078431375 … 0.13333334 0.1254902; … ; 0.047058824 0.039215688 … 0.105882354 0.101960786; 0.050980393 0.047058824 … 0.09803922 0.09803922;;; 0.05882353 0.0627451 … 0.09803922 0.09019608; 0.05490196 0.0627451 … 0.101960786 0.09411765; … ; 0.043137256 0.03529412 … 0.09411765 0.09019608; 0.047058824 0.043137256 … 0.08627451 0.078431375;;; 0.047058824 0.050980393 … 0.05490196 0.047058824; 0.043137256 0.050980393 … 0.05882353 0.050980393; … ; 0.03529412 0.02745098 … 0.21960784 0.20784314; 0.039215688 0.03529412 … 0.18431373 0.18431373;;;; 0.09803922 0.047058824 … 0.40392157 0.37254903; 0.05882353 0.078431375 … 0.40784314 0.37254903; … ; 0.36078432 0.58431375 … 0.3882353 0.37254903; 0.29411766 0.40784314 … 0.36078432 0.36078432;;; 0.15686275 0.09803922 … 0.5176471 0.49411765; 0.14117648 0.14509805 … 0.5137255 0.48235294; … ; 0.44313726 0.65882355 … 0.49803922 0.48235294; 0.34901962 0.45882353 … 0.4745098 0.47058824;;; 0.047058824 0.023529412 … 0.3254902 0.30588236; 0.011764706 0.02745098 … 0.3254902 0.29803923; … ; 0.4392157 0.69411767 … 0.32941177 0.31764707; 0.36078432 0.5137255 … 0.30980393 0.3137255;;;; 0.28627452 0.27058825 … 0.4509804 0.45490196; 0.38431373 0.32941177 … 0.48235294 0.4745098; … ; 0.5294118 0.2784314 … 0.25882354 0.26666668; 0.79607844 0.47058824 … 0.105882354 0.105882354;;; 0.30588236 0.28627452 … 0.4745098 0.47058824; 0.40392157 0.34901962 … 0.4862745 0.47843137; … ; 0.58431375 0.32156864 … 0.25490198 0.25490198; 0.84313726 0.52156866 … 0.105882354 0.101960786;;; 0.29411766 0.27450982 … 0.35686275 0.3529412; 0.44313726 0.38039216 … 0.37254903 0.36862746; … ; 0.6039216 0.3137255 … 0.23137255 0.22745098; 0.8745098 0.5294118 … 0.105882354 0.101960786], Bool[0 0 … 0 0; 0 0 … 1 0; … ; 0 1 … 0 0; 0 0 … 0 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test, Y_test = test_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efedc1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_batched (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function loss_batched(model, X, Y; batchsize=64)\n",
    "  total_loss = 0f0\n",
    "  total_n    = 0\n",
    "  N = size(X, 4)\n",
    "  for i in 1:batchsize:N\n",
    "    j = min(i+batchsize-1, N)\n",
    "    xb = device(X[:,:,:, i:j])\n",
    "    yb = device(Y[:,   i:j])\n",
    "    # 1) oblicz stratę na batchu\n",
    "    l = Flux.crossentropy(model(xb), yb)\n",
    "    # 2) sumuj stratę ważoną liczbą próbek\n",
    "    nb = size(xb, 4)\n",
    "    total_loss += l * nb\n",
    "    total_n    += nb\n",
    "  end\n",
    "  return total_loss / total_n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a037f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "OutOfMemoryError()",
     "output_type": "error",
     "traceback": [
      "OutOfMemoryError()\n",
      "\n",
      "Stacktrace:\n",
      "  [1] GenericMemory\n",
      "    @ .\\boot.jl:516 [inlined]\n",
      "  [2] new_as_memoryref\n",
      "    @ .\\boot.jl:535 [inlined]\n",
      "  [3] Array\n",
      "    @ .\\boot.jl:588 [inlined]\n",
      "  [4] Array\n",
      "    @ .\\boot.jl:594 [inlined]\n",
      "  [5] Array\n",
      "    @ .\\boot.jl:599 [inlined]\n",
      "  [6] similar\n",
      "    @ .\\abstractarray.jl:868 [inlined]\n",
      "  [7] similar\n",
      "    @ .\\abstractarray.jl:867 [inlined]\n",
      "  [8] similar\n",
      "    @ .\\broadcast.jl:224 [inlined]\n",
      "  [9] similar\n",
      "    @ .\\broadcast.jl:223 [inlined]\n",
      " [10] copy\n",
      "    @ .\\broadcast.jl:897 [inlined]\n",
      " [11] materialize\n",
      "    @ .\\broadcast.jl:872 [inlined]\n",
      " [12] _norm_layer_forward(l::BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, x::Array{Float32, 4}; reduce_dims::Vector{Int64}, affine_shape::NTuple{4, Int64})\n",
      "    @ Flux C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\normalise.jl:249\n",
      " [13] _norm_layer_forward\n",
      "    @ C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\normalise.jl:224 [inlined]\n",
      " [14] BatchNorm\n",
      "    @ C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\normalise.jl:352 [inlined]\n",
      " [15] macro expansion\n",
      "    @ C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\basic.jl:68 [inlined]\n",
      " [16] _applychain(layers::Tuple{Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, typeof(flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu), Dropout{Float64, Colon, Random.TaskLocalRNG}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}, x::Array{Float32, 4})\n",
      "    @ Flux C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\basic.jl:68\n",
      " [17] (::Chain{Tuple{Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, typeof(flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu), Dropout{Float64, Colon, Random.TaskLocalRNG}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}})(x::Array{Float32, 4})\n",
      "    @ Flux C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\basic.jl:65\n",
      " [18] loss(model::Chain{Tuple{Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, typeof(flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu), Dropout{Float64, Colon, Random.TaskLocalRNG}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}}, x::Array{Float32, 4}, y::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}, device::Function)\n",
      "    @ Main c:\\Users\\oliwi\\Desktop\\SGH\\masgisterka\\II_sem\\Deep Learning\\projekt_zaliczeniowy\\DeepLearningProjektZaliczeniowy\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X10sZmlsZQ==.jl:4\n",
      " [19] loss(model::Chain{Tuple{Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, typeof(flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu), Dropout{Float64, Colon, Random.TaskLocalRNG}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}}, x::Array{Float32, 4}, y::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}})\n",
      "    @ Main c:\\Users\\oliwi\\Desktop\\SGH\\masgisterka\\II_sem\\Deep Learning\\projekt_zaliczeniowy\\DeepLearningProjektZaliczeniowy\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X10sZmlsZQ==.jl:2\n",
      " [20] top-level scope\n",
      "    @ c:\\Users\\oliwi\\Desktop\\SGH\\masgisterka\\II_sem\\Deep Learning\\projekt_zaliczeniowy\\DeepLearningProjektZaliczeniowy\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X11sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "loss_batched(model_small_convnet, X_test, Y_test; batchsize=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9227f39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy_batched (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function accuracy_batched(model, X, Y; batchsize=64)\n",
    "  total, correct = 0, 0\n",
    "  N = size(X, 4)\n",
    "  for i in 1:batchsize:N\n",
    "    j = min(i+batchsize-1, N)\n",
    "    xb = device(X[:,:,:, i:j])\n",
    "    yb = device(Y[:,   i:j])\n",
    "    ŷ = model(xb)\n",
    "    p = Flux.onecold(ŷ, 0:9)\n",
    "    t = Flux.onecold(yb, 0:9)\n",
    "    correct += sum(p .== t)\n",
    "    total   += length(t)\n",
    "  end\n",
    "  return correct / total\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_batched(model_small_convnet, X_test, Y_test; batchsize=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d98892",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "OutOfMemoryError()",
     "output_type": "error",
     "traceback": [
      "OutOfMemoryError()\n",
      "\n",
      "Stacktrace:\n",
      "  [1] GenericMemory\n",
      "    @ .\\boot.jl:516 [inlined]\n",
      "  [2] new_as_memoryref\n",
      "    @ .\\boot.jl:535 [inlined]\n",
      "  [3] Array\n",
      "    @ .\\boot.jl:588 [inlined]\n",
      "  [4] Array\n",
      "    @ .\\boot.jl:594 [inlined]\n",
      "  [5] Array\n",
      "    @ .\\boot.jl:599 [inlined]\n",
      "  [6] similar\n",
      "    @ .\\abstractarray.jl:868 [inlined]\n",
      "  [7] similar\n",
      "    @ .\\abstractarray.jl:867 [inlined]\n",
      "  [8] similar\n",
      "    @ .\\broadcast.jl:224 [inlined]\n",
      "  [9] similar\n",
      "    @ .\\broadcast.jl:223 [inlined]\n",
      " [10] copy\n",
      "    @ .\\broadcast.jl:897 [inlined]\n",
      " [11] materialize\n",
      "    @ .\\broadcast.jl:872 [inlined]\n",
      " [12] _norm_layer_forward(l::BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, x::Array{Float32, 4}; reduce_dims::Vector{Int64}, affine_shape::NTuple{4, Int64})\n",
      "    @ Flux C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\normalise.jl:249\n",
      " [13] _norm_layer_forward\n",
      "    @ C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\normalise.jl:224 [inlined]\n",
      " [14] BatchNorm\n",
      "    @ C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\normalise.jl:352 [inlined]\n",
      " [15] macro expansion\n",
      "    @ C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\basic.jl:68 [inlined]\n",
      " [16] _applychain(layers::Tuple{Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, typeof(flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu), Dropout{Float64, Colon, Random.TaskLocalRNG}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}, x::Array{Float32, 4})\n",
      "    @ Flux C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\basic.jl:68\n",
      " [17] (::Chain{Tuple{Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, typeof(flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu), Dropout{Float64, Colon, Random.TaskLocalRNG}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}})(x::Array{Float32, 4})\n",
      "    @ Flux C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\basic.jl:65\n",
      " [18] top-level scope\n",
      "    @ c:\\Users\\oliwi\\Desktop\\SGH\\masgisterka\\II_sem\\Deep Learning\\projekt_zaliczeniowy\\DeepLearningProjektZaliczeniowy\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X15sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "# 1) Predykcje i prawdziwe etykiety jako wektory 0–9\n",
    "preds = Flux.onecold(model_small_convnet(X_test), 0:9)\n",
    "trues = Flux.onecold(Y_test,      0:9)\n",
    "\n",
    "# 2) Liczniki\n",
    "class_correct = zeros(Int, 10)\n",
    "class_total   = zeros(Int, 10)\n",
    "\n",
    "# 3) Pętla po wszystkich próbkach\n",
    "for (p, t) in zip(preds, trues)\n",
    "    class_total[t+1]   += 1        \n",
    "    class_correct[t+1] += (p == t)\n",
    "end\n",
    "\n",
    "# 4) Accuracy \n",
    "class_accuracy = class_correct ./ class_total\n",
    "for i in 1:10\n",
    "    @printf(\"%-6s: %5.2f%%  (%4d/%4d)\\n\",\n",
    "        classes[i], 100*class_accuracy[i],\n",
    "        class_correct[i], class_total[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1162fc28",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "OutOfMemoryError()",
     "output_type": "error",
     "traceback": [
      "OutOfMemoryError()\n",
      "\n",
      "Stacktrace:\n",
      "  [1] GenericMemory\n",
      "    @ .\\boot.jl:516 [inlined]\n",
      "  [2] new_as_memoryref\n",
      "    @ .\\boot.jl:535 [inlined]\n",
      "  [3] Array\n",
      "    @ .\\boot.jl:588 [inlined]\n",
      "  [4] Array\n",
      "    @ .\\boot.jl:594 [inlined]\n",
      "  [5] similar\n",
      "    @ .\\array.jl:372 [inlined]\n",
      "  [6] similar\n",
      "    @ .\\abstractarray.jl:824 [inlined]\n",
      "  [7] conv(x::Array{Float32, 4}, w::Array{Float32, 4}, cdims::DenseConvDims{2, 2, 2, 4, 2}; kwargs::@Kwargs{})\n",
      "    @ NNlib C:\\Users\\oliwi\\.julia\\packages\\NNlib\\CGMj3\\src\\conv.jl:86\n",
      "  [8] conv\n",
      "    @ C:\\Users\\oliwi\\.julia\\packages\\NNlib\\CGMj3\\src\\conv.jl:83 [inlined]\n",
      "  [9] (::Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}})(x::Array{Float32, 4})\n",
      "    @ Flux C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\conv.jl:201\n",
      " [10] macro expansion\n",
      "    @ C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\basic.jl:68 [inlined]\n",
      " [11] _applychain(layers::Tuple{Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, typeof(flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu), Dropout{Float64, Colon, Random.TaskLocalRNG}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}, x::Array{Float32, 4})\n",
      "    @ Flux C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\basic.jl:68\n",
      " [12] (::Chain{Tuple{Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(identity), Vector{Float32}, Float32, Vector{Float32}}, typeof(relu), MaxPool{2, 4}, typeof(flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu), Dropout{Float64, Colon, Random.TaskLocalRNG}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}})(x::Array{Float32, 4})\n",
      "    @ Flux C:\\Users\\oliwi\\.julia\\packages\\Flux\\9PibT\\src\\layers\\basic.jl:65\n",
      " [13] top-level scope\n",
      "    @ c:\\Users\\oliwi\\Desktop\\SGH\\masgisterka\\II_sem\\Deep Learning\\projekt_zaliczeniowy\\DeepLearningProjektZaliczeniowy\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X16sZmlsZQ==.jl:3"
     ]
    }
   ],
   "source": [
    "using Images, ImageShow, Plots\n",
    "\n",
    "preds = Flux.onecold(model_small_convnet(X_test), 0:9)\n",
    "trues = Flux.onecold(Y_test,      0:9)\n",
    "\n",
    "function show_predictions(X, trues, preds; n=10)\n",
    "    idxs = rand(1:size(X,4), n)\n",
    "    cols = min(n,5); rows = ceil(Int,n/cols)\n",
    "    plt = plot(layout=(rows,cols), margin=2Plots.mm, xticks=false, yticks=false)\n",
    "    for (i,j) in enumerate(idxs)\n",
    "        raw = X[:,:,:,j] |> cpu\n",
    "        chw = permutedims(raw, (3,2,1))\n",
    "        img = collect(colorview(RGB, chw))\n",
    "        title = \"$(classes[trues[j]+1]) → $(classes[preds[j]+1])\"\n",
    "        plot!(plt[i], img, seriestype=:heatmap,\n",
    "              aspect_ratio=1, axis=false, title=title, titlefontsize=8)\n",
    "    end\n",
    "    display(plt)\n",
    "end\n",
    "\n",
    "show_predictions(X_test, trues, preds; n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52589161",
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase, Plots\n",
    "\n",
    "cm = zeros(Int, 10, 10)\n",
    "for (t,p) in zip(trues, preds)\n",
    "    cm[t+1, p+1] += 1\n",
    "end\n",
    "\n",
    "heatmap(\n",
    "  cm;\n",
    "  xticks=(1:10, classes),\n",
    "  yticks=(1:10, classes),\n",
    "  xlabel=\"Predicted\",\n",
    "  ylabel=\"Actual\",\n",
    "  title=\"Confusion Matrix\",\n",
    "  color = :blues,                # paleta\n",
    "  clims = (0, maximum(cm)),      # skala kolorów od 0 do max\n",
    "  aspect_ratio = 1,              # kwadratowe komórki\n",
    "  right_margin = 10Plots.mm,\n",
    "  annotate = cm                  # wstawia wartości cm[i,j] w każdej komórce\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using StatsBase, Plots\n",
    "\n",
    "# cm = zeros(Int, 10, 10)\n",
    "# for (t,p) in zip(trues, preds)\n",
    "#     cm[t+1, p+1] += 1\n",
    "# end\n",
    "\n",
    "# heatmap(\n",
    "#   cm,\n",
    "#   xticks=(1:10, classes),\n",
    "#   yticks=(1:10, classes),\n",
    "#   xlabel=\"Predicted\",\n",
    "#   ylabel=\"Actual\",\n",
    "#   title=\"Confusion Matrix\",\n",
    "#   right_margin = 10Plots.mm,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
